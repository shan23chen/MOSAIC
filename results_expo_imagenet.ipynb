{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import ast\n",
    "\n",
    "path = 'results/'\n",
    "df0 = pd.read_csv(f'{path}imagenet-1k-256x256_gemma-2b_layer0.csv')\n",
    "df6 = pd.read_csv(f'{path}imagenet-1k-256x256_gemma-2b_layer6.csv')\n",
    "df10 = pd.read_csv(f'{path}imagenet-1k-256x256_gemma-2b_layer10.csv')\n",
    "df12 = pd.read_csv(f'{path}imagenet-1k-256x256_gemma-2b_layer12.csv')\n",
    "df12it = pd.read_csv(f'{path}imagenet-1k-256x256_gemma-2b-it_layer12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image_net_labels.txt as a dictionary\n",
    "with open('image_net_labels.txt', 'r') as f:\n",
    "    image_net_labels = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(df):\n",
    "    label = [image_net_labels[i] for i in df['fine_label']]\n",
    "    df['label'] = label\n",
    "    return df\n",
    "\n",
    "df0 = map_label(df0)\n",
    "df6 = map_label(df6)\n",
    "df10 = map_label(df10)\n",
    "df12 = map_label(df12)\n",
    "df12it = map_label(df12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse 'retrieved_features' column into a list of IDs\n",
    "def parse_retrieved_features(df):\n",
    "    def parse_row(row):\n",
    "        retrieved_features_str = row['retrieved_features']\n",
    "        try:\n",
    "            features_dict = ast.literal_eval(retrieved_features_str)\n",
    "            ids = list(features_dict.keys())\n",
    "            return ids\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    df = df.copy()\n",
    "    df['feature_ids'] = df.apply(parse_row, axis=1)\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.seed()\n",
    "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_shuffled, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Assume df0, df6, df10, df12, df12it are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Parse 'retrieved_features' and collect all unique IDs\n",
    "all_feature_ids_list = []\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    df = parse_retrieved_features(df)\n",
    "    datasets[name] = df\n",
    "    all_feature_ids_list.extend(df['feature_ids'])\n",
    "\n",
    "# Flatten the list of lists to get all feature IDs\n",
    "all_feature_ids = [item for sublist in all_feature_ids_list for item in sublist]\n",
    "\n",
    "# Fit MultiLabelBinarizer to create one-hot encodings\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_feature_ids])  # Fit on the combined list of all feature IDs\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=1, test_size=0.2, seed=None):\n",
    "    f1_scores_list = []\n",
    "    for run_idx in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size=test_size, seed=seed)\n",
    "        X_train = mlb.transform(train_df['feature_ids'])\n",
    "        y_train = train_df['label']\n",
    "        X_test = mlb.transform(test_df['feature_ids'])\n",
    "        y_test = test_df['label']\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "        micro_f1 = f1_score(y_test, predictions, average='micro')\n",
    "        f1_scores_list.append({'macro_f1': macro_f1, 'micro_f1': micro_f1})\n",
    "    return f1_scores_list\n",
    "\n",
    "# Run experiment for each dataset\n",
    "results_list = []\n",
    "for layer_name, df in datasets.items():\n",
    "    layer_name = \"layer\"+layer_name.lstrip('df')\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    seed = np.random.seed() \n",
    "    f1_scores_list = run_experiment(df, n_runs=3)\n",
    "    for run_idx, scores in enumerate(f1_scores_list):\n",
    "        results_list.append({\n",
    "            'Layer': layer_name,\n",
    "            'Run': run_idx + 1,\n",
    "            'Macro F1 Score': scores['macro_f1'],\n",
    "            'Micro F1 Score': scores['micro_f1'],\n",
    "            'seed': seed \n",
    "        })\n",
    "\n",
    "# Create a DataFrame from results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Compute mean and std per layer\n",
    "summary_df = results_df.groupby('Layer').agg({\n",
    "    'Macro F1 Score': ['mean', 'std'],\n",
    "    'Micro F1 Score': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary_df.columns = ['Layer', 'Macro F1 Mean', 'Macro F1 Std', 'Micro F1 Mean', 'Micro F1 Std']\n",
    "\n",
    "# Print out the results\n",
    "print(\"Detailed Results per Run:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nSummary Results per Layer:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Plot Macro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Macro F1 Mean'], yerr=summary_df['Macro F1 Std'], capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Micro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Micro F1 Mean'], yerr=summary_df['Micro F1 Std'], capsize=5, color='lightgreen')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Micro F1 Score')\n",
    "plt.title('Micro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'retrieved_features' column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['retrieved_features'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=None):\n",
    "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_shuffled, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to perform TF-IDF classification and calculate F1 scores\n",
    "def tfidf_classification(train_texts, train_labels, test_texts, test_labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    return macro_f1, micro_f1\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=1, test_size=0.2, seed=None):\n",
    "    f1_scores_list = []\n",
    "    for run_idx in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size=test_size, seed=seed)\n",
    "        train_texts = train_df['text']\n",
    "        train_labels = train_df['label']\n",
    "        test_texts = test_df['text']\n",
    "        test_labels = test_df['label']\n",
    "        \n",
    "        macro_f1, micro_f1 = tfidf_classification(train_texts, train_labels, test_texts, test_labels)\n",
    "        f1_scores_list.append({'macro_f1': macro_f1, 'micro_f1': micro_f1})\n",
    "    return f1_scores_list\n",
    "\n",
    "# Assume df0, df6, df10, df12, df12it are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Convert 'retrieved_features' to string and ensure 'label' column exists for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset and collect results\n",
    "results_list = []\n",
    "for layer_name, df in datasets.items():\n",
    "    layer_name = \"layer \"+layer_name\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    f1_scores_list = run_experiment(df, n_runs=3, seed=42)\n",
    "    for run_idx, scores in enumerate(f1_scores_list):\n",
    "        results_list.append({\n",
    "            'Layer': layer_name,\n",
    "            'Run': run_idx + 1,\n",
    "            'Macro F1 Score': scores['macro_f1'],\n",
    "            'Micro F1 Score': scores['micro_f1']\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Compute mean and std per layer\n",
    "summary_df = results_df.groupby('Layer').agg({\n",
    "    'Macro F1 Score': ['mean', 'std'],\n",
    "    'Micro F1 Score': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary_df.columns = ['Layer', 'Macro F1 Mean', 'Macro F1 Std', 'Micro F1 Mean', 'Micro F1 Std']\n",
    "\n",
    "# Print out the results\n",
    "print(\"Detailed Results per Run:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nSummary Results per Layer:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Plot Macro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Macro F1 Mean'], yerr=summary_df['Macro F1 Std'], capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Micro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Micro F1 Mean'], yerr=summary_df['Micro F1 Std'], capsize=5, color='lightgreen')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Micro F1 Score')\n",
    "plt.title('Micro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'retrieved_features' column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['retrieved_features'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=None):\n",
    "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_shuffled, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to perform TF-IDF classification\n",
    "def tfidf_classification(train_texts, train_labels, test_texts, test_labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=3, test_size=0.2, seed=None):\n",
    "    macro_f1_scores = []\n",
    "    for _ in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size=test_size, seed=seed)\n",
    "        train_texts = train_df['text']\n",
    "        train_labels = train_df['label']\n",
    "        test_texts = test_df['text']\n",
    "        test_labels = test_df['label']\n",
    "        \n",
    "        macro_f1 = tfidf_classification(train_texts, train_labels, test_texts, test_labels)\n",
    "        macro_f1_scores.append(macro_f1)\n",
    "    return macro_f1_scores\n",
    "\n",
    "# Assume df12it, df0, df6, df10, df12 are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Convert 'retrieved_features' to string and ensure 'label' column exists for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset\n",
    "layer_macro_f1_scores = {}\n",
    "for layer_name, df in datasets.items():\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    macro_f1_scores = run_experiment(df, n_runs=1, seed=42)\n",
    "    layer_macro_f1_scores[layer_name] = macro_f1_scores\n",
    "\n",
    "# Perform t-tests between each pair of layers\n",
    "layer_pairs = list(combinations(datasets.keys(), 2))\n",
    "t_test_results = {}\n",
    "for (layer1, layer2) in layer_pairs:\n",
    "    scores1 = layer_macro_f1_scores[layer1]\n",
    "    scores2 = layer_macro_f1_scores[layer2]\n",
    "    t_stat, p_value = ttest_ind(scores1, scores2)\n",
    "    t_test_results[(layer1, layer2)] = {'t_stat': t_stat, 'p_value': p_value}\n",
    "\n",
    "# Plot results\n",
    "layers = sorted(layer_macro_f1_scores.keys())\n",
    "means = [np.mean(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "stds = [np.std(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(layers, means, yerr=stds, capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Layer Macro F1 Scores:\")\n",
    "for layer in layers:\n",
    "    scores = layer_macro_f1_scores[layer]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{layer}: Mean = {mean_score:.4f}, Std = {std_score:.4f}, Scores = {scores}\")\n",
    "\n",
    "print(\"\\nT-Test Results:\")\n",
    "for (layer1, layer2), result in t_test_results.items():\n",
    "    t_stat = result['t_stat']\n",
    "    p_value = result['p_value']\n",
    "    print(f\"{layer1} vs {layer2}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counts(df):\n",
    "    \n",
    "    all_keys = []\n",
    "    for features in df['retrieved_features']:\n",
    "        feature_dict = eval(features)  # Convert string to dictionary\n",
    "        all_keys.extend(feature_dict.keys())\n",
    "        \n",
    "    x = Counter(all_keys).most_common()\n",
    "    print('before:',len(x))\n",
    "    # now i want to remove key value > 1000 from feature_dict\n",
    "    x = dict(x)\n",
    "    for key in list(x.keys()):\n",
    "        if x[key] > 1000:\n",
    "            del x[key]\n",
    "    print('after:',len(x))\n",
    "    print('=====')\n",
    "    return x\n",
    "\n",
    "# 265\n",
    "# 1186\n",
    "# 2213\n",
    "# 1823\n",
    "# 2016\n",
    "\n",
    "feature_counts0 = get_feature_counts(df0)\n",
    "feature_counts6 = get_feature_counts(df6)\n",
    "feature_counts10 = get_feature_counts(df10)\n",
    "feature_counts12 = get_feature_counts(df12)\n",
    "feature_counts12it = get_feature_counts(df12it)\n",
    "\n",
    "def clean_features(df, feature_counts):\n",
    "    new_features = []\n",
    "    for features in df['retrieved_features']:\n",
    "        features = eval(features)\n",
    "        features = {k:v for k,v in features.items() if k in feature_counts}\n",
    "        new_features.append(features)\n",
    "    df['cleaned_features'] = new_features\n",
    "    return df\n",
    "\n",
    "df0 = clean_features(df0, feature_counts0)\n",
    "df6 = clean_features(df6, feature_counts6)\n",
    "df10 = clean_features(df10, feature_counts10)\n",
    "df12 = clean_features(df12, feature_counts12)\n",
    "df12it = clean_features(df12it, feature_counts12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'retrieved_features' column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['cleaned_features'].astype(str)\n",
    "    return df\n",
    "# Assume df12it, df0, df6, df10, df12 are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Convert 'retrieved_features' to string and ensure 'label' column exists for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset\n",
    "layer_macro_f1_scores = {}\n",
    "for layer_name, df in datasets.items():\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    macro_f1_scores = run_experiment(df, n_runs=1, seed=42)\n",
    "    layer_macro_f1_scores[layer_name] = macro_f1_scores\n",
    "\n",
    "# Perform t-tests between each pair of layers\n",
    "layer_pairs = list(combinations(datasets.keys(), 2))\n",
    "t_test_results = {}\n",
    "for (layer1, layer2) in layer_pairs:\n",
    "    scores1 = layer_macro_f1_scores[layer1]\n",
    "    scores2 = layer_macro_f1_scores[layer2]\n",
    "    t_stat, p_value = ttest_ind(scores1, scores2)\n",
    "    t_test_results[(layer1, layer2)] = {'t_stat': t_stat, 'p_value': p_value}\n",
    "\n",
    "# Plot results\n",
    "layers = sorted(layer_macro_f1_scores.keys())\n",
    "means = [np.mean(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "stds = [np.std(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(layers, means, yerr=stds, capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Layer Macro F1 Scores:\")\n",
    "for layer in layers:\n",
    "    scores = layer_macro_f1_scores[layer]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    print(f\"{layer}: Mean = {mean_score:.4f}, Std = {std_score:.4f}, Scores = {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counts(df):\n",
    "    \n",
    "    all_keys = []\n",
    "    for features in df['retrieved_features']:\n",
    "        feature_dict = eval(features)  # Convert string to dictionary\n",
    "        all_keys.extend(feature_dict.keys())\n",
    "        \n",
    "    x = Counter(all_keys).most_common()\n",
    "    print('before:',len(x))\n",
    "    # now i want to remove key value > 1000 from feature_dict\n",
    "    x = dict(x)\n",
    "    for key in list(x.keys()):\n",
    "        if x[key] > 1000:\n",
    "            del x[key]\n",
    "    print('after:',len(x))\n",
    "    print('=====')\n",
    "    return x\n",
    "\n",
    "# 265\n",
    "# 1186\n",
    "# 2213\n",
    "# 1823\n",
    "# 2016\n",
    "\n",
    "feature_counts0 = get_feature_counts(df0)\n",
    "feature_counts6 = get_feature_counts(df6)\n",
    "feature_counts10 = get_feature_counts(df10)\n",
    "feature_counts12 = get_feature_counts(df12)\n",
    "# feature_counts12it = get_feature_counts(df12it)\n",
    "\n",
    "def clean_features(df, feature_counts):\n",
    "    new_features = []\n",
    "    for features in df['retrieved_features']:\n",
    "        features = eval(features)\n",
    "        features = {k:v for k,v in features.items() if k in feature_counts}\n",
    "        new_features.append(features)\n",
    "    df['cleaned_features'] = new_features\n",
    "    return df\n",
    "\n",
    "df0 = clean_features(df0, feature_counts0)\n",
    "df6 = clean_features(df6, feature_counts6)\n",
    "df10 = clean_features(df10, feature_counts10)\n",
    "df12 = clean_features(df12, feature_counts12)\n",
    "# df12it = clean_features(df12it, feature_counts12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all above numbers out of 10000\n",
    "print('layer0:',sum([i!={} for i in df0['cleaned_features']])/len(df0))\n",
    "print('layer6:',sum([i!={} for i in df6['cleaned_features']])/len(df6))\n",
    "print('layer10:',sum([i!={} for i in df10['cleaned_features']])/len(df10))\n",
    "print('layer12:',sum([i!={} for i in df12['cleaned_features']])/len(df12))\n",
    "# print('layer12it:',sum([i!={} for i in df12it['cleaned_features']])/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a function where coarse_label is the same, check which key in cleaned_features appear the most top 5\n",
    "\n",
    "def get_top5_features(df, coi='label'):\n",
    "    top5_features = []\n",
    "    labels = set(df[coi])\n",
    "    for i in labels:\n",
    "        df_coarse = df[df[coi] == i]\n",
    "        all_keys = []\n",
    "        for features in df_coarse['cleaned_features']:\n",
    "            all_keys.extend(features.keys())\n",
    "        # iterate all and combine into one dict\n",
    "        all_features = {}\n",
    "        for features in df['cleaned_features']:\n",
    "            all_features.update(features)\n",
    "        x = Counter(all_keys).most_common()\n",
    "        top5_features.append({i:x[:5]})\n",
    "        print(i, x[:5], 'This is out of:', len(df[coi])/len(labels))\n",
    "        print([all_features[i[0]] for i in x[:5]])\n",
    "    return top5_features\n",
    "\n",
    "top5_features0 = get_top5_features(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12 = get_top5_features(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12it = get_top5_features(df12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12 = get_top5_features(df12, coi='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12it = get_top5_features(df12it, coi='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# get these vectors, get their pre vit embedding, post vit embedding, and the difference, and top five activation patterns\n",
    "# then plot them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to convert retrieved_features column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df['text'] = df['retrieved_features'].apply(lambda x: str(x))\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=42):\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to perform TF-IDF classification\n",
    "def tfidf_classification(train_texts, train_labels, test_texts, test_labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, train_labels)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=5, test_size=0.2, seed=42):\n",
    "    macro_f1_scores = []\n",
    "    for _ in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size, seed)\n",
    "        train_texts = train_df['text']\n",
    "        train_labels = train_df['label']\n",
    "        test_texts = test_df['text']\n",
    "        test_labels = test_df['label']\n",
    "        \n",
    "        macro_f1 = tfidf_classification(train_texts, train_labels, test_texts, test_labels)\n",
    "        macro_f1_scores.append(macro_f1)\n",
    "    return macro_f1_scores\n",
    "\n",
    "# Assume df12it, df0, df6, df10, df12 are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df12it': df12it,\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12\n",
    "}\n",
    "\n",
    "# Convert retrieved_features to string for each dataset\n",
    "for name, df in datasets.items():\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset\n",
    "layer_macro_f1_scores = {}\n",
    "for layer_name, df in datasets.items():\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    macro_f1_scores = run_experiment(df, n_runs=5)\n",
    "    layer_macro_f1_scores[layer_name] = macro_f1_scores\n",
    "\n",
    "# Perform t-test\n",
    "layer_pairs = [(i, j) for i in datasets.keys() for j in datasets.keys() if i < j]\n",
    "t_test_results = {}\n",
    "for (layer1, layer2) in layer_pairs:\n",
    "    t_stat, p_value = ttest_ind(layer_macro_f1_scores[layer1], layer_macro_f1_scores[layer2])\n",
    "    t_test_results[(layer1, layer2)] = (t_stat, p_value)\n",
    "\n",
    "# Plot results\n",
    "layers = list(layer_macro_f1_scores.keys())\n",
    "means = [np.mean(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "stds = [np.std(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(layers, means, yerr=stds, fmt='o', capsize=5, capthick=2, ecolor='red')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Layer Macro F1 Scores:\", layer_macro_f1_scores)\n",
    "print(\"T-Test Results:\", t_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to convert retrieved_features column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df['text'] = df['retrieved_features'].apply(lambda x: str(x))\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=42):\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to perform TF-IDF classification\n",
    "def tfidf_classification(train_texts, train_labels, test_texts, test_labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, train_labels)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    return macro_f1\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=5, test_size=0.2, seed=42):\n",
    "    macro_f1_scores = []\n",
    "    for _ in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size, seed)\n",
    "        train_texts = train_df['text']\n",
    "        train_labels = train_df['label']\n",
    "        test_texts = test_df['text']\n",
    "        test_labels = test_df['label']\n",
    "        \n",
    "        macro_f1 = tfidf_classification(train_texts, train_labels, test_texts, test_labels)\n",
    "        macro_f1_scores.append(macro_f1)\n",
    "    return macro_f1_scores\n",
    "\n",
    "# Assume df12it, df0, df6, df10, df12 are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df12it': df12it,\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12\n",
    "}\n",
    "\n",
    "# Convert retrieved_features to string for each dataset\n",
    "for name, df in datasets.items():\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset\n",
    "layer_macro_f1_scores = {}\n",
    "for layer_name, df in datasets.items():\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    macro_f1_scores = run_experiment(df, n_runs=5)\n",
    "    layer_macro_f1_scores[layer_name] = macro_f1_scores\n",
    "\n",
    "# Perform t-test\n",
    "layer_pairs = [(i, j) for i in datasets.keys() for j in datasets.keys() if i < j]\n",
    "t_test_results = {}\n",
    "for (layer1, layer2) in layer_pairs:\n",
    "    t_stat, p_value = ttest_ind(layer_macro_f1_scores[layer1], layer_macro_f1_scores[layer2])\n",
    "    t_test_results[(layer1, layer2)] = (t_stat, p_value)\n",
    "\n",
    "# Plot results\n",
    "layers = list(layer_macro_f1_scores.keys())\n",
    "means = [np.mean(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "stds = [np.std(layer_macro_f1_scores[layer]) for layer in layers]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(layers, means, yerr=stds, fmt='o', capsize=5, capthick=2, ecolor='red')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "print(\"Layer Macro F1 Scores:\", layer_macro_f1_scores)\n",
    "print(\"T-Test Results:\", t_test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
