{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import ast\n",
    "\n",
    "path = 'results/'\n",
    "df0 = pd.read_csv(f'{path}cifar100-enriched_gemma-2b_layer0.csv')\n",
    "df6 = pd.read_csv(f'{path}cifar100-enriched_gemma-2b_layer6.csv')\n",
    "df10 = pd.read_csv(f'{path}cifar100-enriched_gemma-2b_layer10.csv')\n",
    "df12 = pd.read_csv(f'{path}cifar100-enriched_gemma-2b_layer12.csv')\n",
    "df12it = pd.read_csv(f'{path}cifar100-enriched_gemma-2b-it_layer12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12['retrieved_features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(df):\n",
    "    label = [i for i in df['coarse_label']]\n",
    "    df['label'] = label\n",
    "    return df\n",
    "\n",
    "df0 = map_label(df0)\n",
    "df6 = map_label(df6)\n",
    "df10 = map_label(df10)\n",
    "df12 = map_label(df12)\n",
    "df12it = map_label(df12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse 'retrieved_features' column into a list of IDs\n",
    "def parse_retrieved_features(df):\n",
    "    def parse_row(row):\n",
    "        retrieved_features_str = row['retrieved_features']\n",
    "        try:\n",
    "            features_dict = ast.literal_eval(retrieved_features_str)\n",
    "            ids = list(features_dict.keys())\n",
    "            return ids\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []\n",
    "    df = df.copy()\n",
    "    df['feature_ids'] = df.apply(parse_row, axis=1)\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.seed()\n",
    "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_shuffled, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Assume df0, df6, df10, df12, df12it are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Parse 'retrieved_features' and collect all unique IDs\n",
    "all_feature_ids_list = []\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    df = parse_retrieved_features(df)\n",
    "    datasets[name] = df\n",
    "    all_feature_ids_list.extend(df['feature_ids'])\n",
    "\n",
    "# Flatten the list of lists to get all feature IDs\n",
    "all_feature_ids = [item for sublist in all_feature_ids_list for item in sublist]\n",
    "\n",
    "# Fit MultiLabelBinarizer to create one-hot encodings\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_feature_ids])  # Fit on the combined list of all feature IDs\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=3, test_size=0.2, seed=None):\n",
    "    f1_scores_list = []\n",
    "    for run_idx in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size=test_size, seed=seed)\n",
    "        X_train = mlb.transform(train_df['feature_ids'])\n",
    "        y_train = train_df['label']\n",
    "        X_test = mlb.transform(test_df['feature_ids'])\n",
    "        y_test = test_df['label']\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "        micro_f1 = f1_score(y_test, predictions, average='micro')\n",
    "        f1_scores_list.append({'macro_f1': macro_f1, 'micro_f1': micro_f1})\n",
    "    return f1_scores_list\n",
    "\n",
    "# Run experiment for each dataset\n",
    "results_list = []\n",
    "for layer_name, df in datasets.items():\n",
    "    layer_name = \"layer \"+layer_name.lstrip('df')\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    seed = np.random.seed() \n",
    "    f1_scores_list = run_experiment(df, n_runs=3)\n",
    "    for run_idx, scores in enumerate(f1_scores_list):\n",
    "        results_list.append({\n",
    "            'Layer': layer_name,\n",
    "            'Run': run_idx + 1,\n",
    "            'Macro F1 Score': scores['macro_f1'],\n",
    "            'Micro F1 Score': scores['micro_f1'],\n",
    "            'seed': seed \n",
    "        })\n",
    "\n",
    "# Create a DataFrame from results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Compute mean and std per layer\n",
    "summary_df = results_df.groupby('Layer').agg({\n",
    "    'Macro F1 Score': ['mean', 'std'],\n",
    "    'Micro F1 Score': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary_df.columns = ['Layer', 'Macro F1 Mean', 'Macro F1 Std', 'Micro F1 Mean', 'Micro F1 Std']\n",
    "\n",
    "# Print out the results\n",
    "print(\"Detailed Results per Run:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nSummary Results per Layer:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Plot Macro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Macro F1 Mean'], yerr=summary_df['Macro F1 Std'], capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Micro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Micro F1 Mean'], yerr=summary_df['Micro F1 Std'], capsize=5, color='lightgreen')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Micro F1 Score')\n",
    "plt.title('Micro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom order for the Layer column\n",
    "layer_order = ['layer 0', 'layer 6', 'layer 10', 'layer 12', 'layer 12it']\n",
    "\n",
    "# Convert the Layer column to a categorical type with the specified order\n",
    "summary_df['Layer'] = pd.Categorical(summary_df['Layer'], categories=layer_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the Layer column\n",
    "summary_df = summary_df.sort_values('Layer')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'retrieved_features' column to string\n",
    "def convert_retrieved_features_to_string(df):\n",
    "    df = df.copy()\n",
    "    df['text'] = df['retrieved_features'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Function to shuffle and split data\n",
    "def shuffle_and_split(df, test_size=0.2, seed=None):\n",
    "    df_shuffled = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(df_shuffled, test_size=test_size, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Function to perform TF-IDF classification and calculate F1 scores\n",
    "def tfidf_classification(train_texts, train_labels, test_texts, test_labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, train_labels)\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    return macro_f1, micro_f1\n",
    "\n",
    "# Function to run the experiment multiple times\n",
    "def run_experiment(df, n_runs=3, test_size=0.2, seed=None):\n",
    "    f1_scores_list = []\n",
    "    for run_idx in range(n_runs):\n",
    "        train_df, test_df = shuffle_and_split(df, test_size=test_size, seed=seed)\n",
    "        train_texts = train_df['text']\n",
    "        train_labels = train_df['label']\n",
    "        test_texts = test_df['text']\n",
    "        test_labels = test_df['label']\n",
    "        \n",
    "        macro_f1, micro_f1 = tfidf_classification(train_texts, train_labels, test_texts, test_labels)\n",
    "        f1_scores_list.append({'macro_f1': macro_f1, 'micro_f1': micro_f1})\n",
    "    return f1_scores_list\n",
    "\n",
    "# Assume df0, df6, df10, df12, df12it are already loaded as pandas DataFrames\n",
    "datasets = {\n",
    "    'df0': df0,\n",
    "    'df6': df6,\n",
    "    'df10': df10,\n",
    "    'df12': df12,\n",
    "    'df12it': df12it\n",
    "}\n",
    "\n",
    "# Convert 'retrieved_features' to string and ensure 'label' column exists for each dataset\n",
    "for name, df in datasets.items():\n",
    "    if 'retrieved_features' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'retrieved_features' column.\")\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(f\"Dataset '{name}' does not have 'label' column.\")\n",
    "    datasets[name] = convert_retrieved_features_to_string(df)\n",
    "\n",
    "# Run experiment for each dataset and collect results\n",
    "results_list = []\n",
    "for layer_name, df in datasets.items():\n",
    "    layer_name = \"layer \"+layer_name.lstrip('df')\n",
    "    print(f\"Running experiment for {layer_name}\")\n",
    "    f1_scores_list = run_experiment(df, n_runs=3, seed=42)\n",
    "    for run_idx, scores in enumerate(f1_scores_list):\n",
    "        results_list.append({\n",
    "            'Layer': layer_name,\n",
    "            'Run': run_idx + 1,\n",
    "            'Macro F1 Score': scores['macro_f1'],\n",
    "            'Micro F1 Score': scores['micro_f1']\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from results_list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Compute mean and std per layer\n",
    "summary_df = results_df.groupby('Layer').agg({\n",
    "    'Macro F1 Score': ['mean', 'std'],\n",
    "    'Micro F1 Score': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary_df.columns = ['Layer', 'Macro F1 Mean', 'Macro F1 Std', 'Micro F1 Mean', 'Micro F1 Std']\n",
    "\n",
    "# Print out the results\n",
    "print(\"Detailed Results per Run:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nSummary Results per Layer:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Plot Macro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Macro F1 Mean'], yerr=summary_df['Macro F1 Std'], capsize=5, color='skyblue')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.title('Macro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Micro F1 Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(summary_df['Layer'], summary_df['Micro F1 Mean'], yerr=summary_df['Micro F1 Std'], capsize=5, color='lightgreen')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Micro F1 Score')\n",
    "plt.title('Micro F1 Score by Layer with Error Bars')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom order for the Layer column\n",
    "layer_order = ['layer 0', 'layer 6', 'layer 10', 'layer 12', 'layer 12it']\n",
    "\n",
    "# Convert the Layer column to a categorical type with the specified order\n",
    "summary_df['Layer'] = pd.Categorical(summary_df['Layer'], categories=layer_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by the Layer column\n",
    "summary_df = summary_df.sort_values('Layer')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_counts(df):\n",
    "    \n",
    "    all_keys = []\n",
    "    for features in df['retrieved_features']:\n",
    "        feature_dict = eval(features)  # Convert string to dictionary\n",
    "        all_keys.extend(feature_dict.keys())\n",
    "        \n",
    "    x = Counter(all_keys).most_common()\n",
    "    print('before:',len(x))\n",
    "    # now i want to remove key value > 1000 from feature_dict\n",
    "    x = dict(x)\n",
    "    for key in list(x.keys()):\n",
    "        if x[key] > 1000:\n",
    "            del x[key]\n",
    "    print('after:',len(x))\n",
    "    print('=====')\n",
    "    return x\n",
    "\n",
    "# 265\n",
    "# 1186\n",
    "# 2213\n",
    "# 1823\n",
    "# 2016\n",
    "\n",
    "feature_counts0 = get_feature_counts(df0)\n",
    "feature_counts6 = get_feature_counts(df6)\n",
    "feature_counts10 = get_feature_counts(df10)\n",
    "feature_counts12 = get_feature_counts(df12)\n",
    "feature_counts12it = get_feature_counts(df12it)\n",
    "\n",
    "def clean_features(df, feature_counts):\n",
    "    new_features = []\n",
    "    for features in df['retrieved_features']:\n",
    "        features = eval(features)\n",
    "        features = {k:v for k,v in features.items() if k in feature_counts}\n",
    "        new_features.append(features)\n",
    "    df['cleaned_features'] = new_features\n",
    "    return df\n",
    "\n",
    "df0 = clean_features(df0, feature_counts0)\n",
    "df6 = clean_features(df6, feature_counts6)\n",
    "df10 = clean_features(df10, feature_counts10)\n",
    "df12 = clean_features(df12, feature_counts12)\n",
    "df12it = clean_features(df12it, feature_counts12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all above numbers out of 10000\n",
    "print('layer0:',sum([i!={} for i in df0['cleaned_features']])/10000)\n",
    "print('layer6:',sum([i!={} for i in df6['cleaned_features']])/10000)\n",
    "print('layer10:',sum([i!={} for i in df10['cleaned_features']])/10000)\n",
    "print('layer12:',sum([i!={} for i in df12['cleaned_features']])/10000)\n",
    "print('layer12it:',sum([i!={} for i in df12it['cleaned_features']])/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a function where coarse_label is the same, check which key in cleaned_features appear the most top 5\n",
    "\n",
    "def get_top5_features(df, coi='fine_label', topn=5):\n",
    "    top5_features = []\n",
    "    labels = set(df[coi])\n",
    "    for i in labels:\n",
    "        df_coarse = df[df[coi] == i]\n",
    "        all_keys = []\n",
    "        for features in df_coarse['cleaned_features']:\n",
    "            all_keys.extend(features.keys())\n",
    "        # iterate all and combine into one dict\n",
    "        all_features = {}\n",
    "        for features in df['cleaned_features']:\n",
    "            all_features.update(features)\n",
    "        x = Counter(all_keys).most_common()\n",
    "        top5_features.append({i:x[:5]})\n",
    "        print(i, x[:5], 'This is out of:', 10000/len(labels))\n",
    "        print([all_features[i[0]] for i in x[:topn]])\n",
    "    return top5_features\n",
    "\n",
    "top5_features0 = get_top5_features(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12 = get_top5_features(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12 = get_top5_features(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12it = get_top5_features(df12it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12 = get_top5_features(df12, coi='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_features12it = get_top5_features(df12it, coi='coarse_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top5_features(df12it, topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# get these vectors, get their pre vit embedding, post vit embedding, and the difference, and top five activation patterns\n",
    "# then plot them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "import pandas as pd\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the environment variable to use only GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:1\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "release = \"gemma-2b-it\"\n",
    "layer = 12\n",
    "\n",
    "def create_vector(size, indices):\n",
    "    \"\"\"\n",
    "    Create a vector of given size where the elements at the specified indices are 1 and the rest are 0.\n",
    "\n",
    "    Parameters:\n",
    "    size (int): The size of the vector.\n",
    "    indices (list of int): The indices to be set to 1.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The resulting vector.\n",
    "    \"\"\"\n",
    "    vector = torch.zeros(1, size)\n",
    "    for index in indices:\n",
    "        vector[0, index] = 1\n",
    "    return vector.to(device)\n",
    "\n",
    "def get_activated_sae(release, layer, df):\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=f\"{release}-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_post\",\n",
    "        device=device\n",
    "    )\n",
    "    # default dict of dict where all keys are id from 0-cfg_dict['d_sae'], and it contains count all start from 0 and the sae decode vector of the id\n",
    "    sae_dict = {i:{'count':0, 'vector':create_vector(cfg_dict['d_sae'], [i])@sae.W_dec} for i in range(cfg_dict['d_sae'])}\n",
    "    for i in df['cleaned_features']:\n",
    "        if i != {}:\n",
    "            for sae_id in i.keys():\n",
    "                sae_dict[int(sae_id)]['count'] += 1\n",
    "    return sae_dict\n",
    "\n",
    "dict_12 = get_activated_sae(release, layer, df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [dict_12[i]['count'] for i in dict_12]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(range(len(counts)), counts)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assuming sae_dict is obtained from get_activated_sae\n",
    "sae_dict = get_activated_sae(release, layer, df12)\n",
    "\n",
    "# Extract vectors and counts\n",
    "vectors = [sae_dict[key]['vector'].detach().cpu().numpy().flatten() for key in sae_dict]\n",
    "counts = [sae_dict[key]['count'] for key in sae_dict]\n",
    "\n",
    "# Perform UMAP\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(vectors)\n",
    "\n",
    "# Create a custom colormap\n",
    "cmap = plt.cm.viridis\n",
    "norm = mcolors.Normalize(vmin=min(counts), vmax=max(counts))\n",
    "\n",
    "# Plot the UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=counts, cmap=cmap, norm=norm, s=100)\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Count')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.title('UMAP of SAE Vectors')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vectors and counts\n",
    "vectors = [sae_dict[key]['vector'].detach().cpu().numpy().flatten() for key in sae_dict]\n",
    "counts = [sae_dict[key]['count'] for key in sae_dict]\n",
    "\n",
    "# Perform UMAP\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(vectors)\n",
    "\n",
    "# Create a custom colormap\n",
    "cmap = plt.cm.viridis\n",
    "norm = mcolors.Normalize(vmin=min(counts), vmax=max(counts))\n",
    "\n",
    "# Create an array for alpha values based on counts\n",
    "alphas = np.array([1 if count >= 20 else 0 for count in counts])\n",
    "\n",
    "# Plot the UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=counts, cmap=cmap, norm=norm, s=100, alpha=alphas)\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Count')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.title('UMAP of SAE Vectors')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
